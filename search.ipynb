{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting clip@ git+https://github.com/openai/CLIP.git@a1d071733d7111c9c014f024669f959182114e33 (from -r requirements.txt (line 1))\n",
      "  Cloning https://github.com/openai/CLIP.git (to revision a1d071733d7111c9c014f024669f959182114e33) to /private/var/folders/4_/051f5l115s33r4zvqhcsst0r0000gn/T/pip-install-_8jc7wb7/clip_31378a0dad594d179af4de51c841b944\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /private/var/folders/4_/051f5l115s33r4zvqhcsst0r0000gn/T/pip-install-_8jc7wb7/clip_31378a0dad594d179af4de51c841b944\n",
      "  Running command git rev-parse -q --verify 'sha^a1d071733d7111c9c014f024669f959182114e33'\n",
      "  Running command git fetch -q https://github.com/openai/CLIP.git a1d071733d7111c9c014f024669f959182114e33\n",
      "  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: filelock==3.13.4 in ./venv/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (3.13.4)\n",
      "Requirement already satisfied: fsspec==2024.3.1 in ./venv/lib/python3.11/site-packages (from -r requirements.txt (line 3)) (2024.3.1)\n",
      "Requirement already satisfied: ftfy==6.2.0 in ./venv/lib/python3.11/site-packages (from -r requirements.txt (line 4)) (6.2.0)\n",
      "Requirement already satisfied: Jinja2==3.1.3 in ./venv/lib/python3.11/site-packages (from -r requirements.txt (line 5)) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe==2.1.5 in ./venv/lib/python3.11/site-packages (from -r requirements.txt (line 6)) (2.1.5)\n",
      "Requirement already satisfied: mpmath==1.3.0 in ./venv/lib/python3.11/site-packages (from -r requirements.txt (line 7)) (1.3.0)\n",
      "Requirement already satisfied: networkx==3.3 in ./venv/lib/python3.11/site-packages (from -r requirements.txt (line 8)) (3.3)\n",
      "Requirement already satisfied: numpy==1.26.4 in ./venv/lib/python3.11/site-packages (from -r requirements.txt (line 9)) (1.26.4)\n",
      "Requirement already satisfied: pillow==10.3.0 in ./venv/lib/python3.11/site-packages (from -r requirements.txt (line 10)) (10.3.0)\n",
      "Requirement already satisfied: regex==2023.12.25 in ./venv/lib/python3.11/site-packages (from -r requirements.txt (line 11)) (2023.12.25)\n",
      "Requirement already satisfied: sympy==1.12 in ./venv/lib/python3.11/site-packages (from -r requirements.txt (line 12)) (1.12)\n",
      "Requirement already satisfied: torch==2.2.2 in ./venv/lib/python3.11/site-packages (from -r requirements.txt (line 13)) (2.2.2)\n",
      "Requirement already satisfied: torchvision==0.17.2 in ./venv/lib/python3.11/site-packages (from -r requirements.txt (line 14)) (0.17.2)\n",
      "Requirement already satisfied: tqdm==4.66.2 in ./venv/lib/python3.11/site-packages (from -r requirements.txt (line 15)) (4.66.2)\n",
      "Requirement already satisfied: typing_extensions==4.11.0 in ./venv/lib/python3.11/site-packages (from -r requirements.txt (line 16)) (4.11.0)\n",
      "Requirement already satisfied: wcwidth==0.2.13 in ./venv/lib/python3.11/site-packages (from -r requirements.txt (line 17)) (0.2.13)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas \n",
    "\n",
    "df = pandas.read_csv('./mincult-train/train.csv', sep=';')\n",
    "df = df[:100]\n",
    "df['path'] = './mincult-train/train/'+ df['object_id'].astype(str) + '/' + df['img_name']\n",
    "df['embedding_path'] = df['path']+'.embedding'\n",
    "df['embedding_path_ru'] = df['path']+'.embedding'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>object_id</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>group</th>\n",
       "      <th>img_name</th>\n",
       "      <th>path</th>\n",
       "      <th>embedding_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10669820</td>\n",
       "      <td>Водолей - коник (фрагмент - голова)</td>\n",
       "      <td>сероглиняный, лепной, со сплошным белым ангобо...</td>\n",
       "      <td>Археология</td>\n",
       "      <td>7862029.jpg</td>\n",
       "      <td>./mincult-train/train/10669820/7862029.jpg</td>\n",
       "      <td>./mincult-train/train/10669820/7862029.jpg.emb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4489444</td>\n",
       "      <td>Винтовка «Самозарядная винтовка Токарева» (мет...</td>\n",
       "      <td>На стволе имеется надульник, на  торце которог...</td>\n",
       "      <td>Оружие</td>\n",
       "      <td>9461061.jpg</td>\n",
       "      <td>./mincult-train/train/4489444/9461061.jpg</td>\n",
       "      <td>./mincult-train/train/4489444/9461061.jpg.embe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8722586</td>\n",
       "      <td>Инструмент. Калибр-скоба</td>\n",
       "      <td>Прямоугольная пластина с усечёнными углами и д...</td>\n",
       "      <td>Прочие</td>\n",
       "      <td>5095122.jpg</td>\n",
       "      <td>./mincult-train/train/8722586/5095122.jpg</td>\n",
       "      <td>./mincult-train/train/8722586/5095122.jpg.embe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3712248</td>\n",
       "      <td>Судомодель. НИС \"Космонавт  Виктор Пацаев\".</td>\n",
       "      <td>Корпус модели, надстройки, шлюпки выполнены и...</td>\n",
       "      <td>Прочие</td>\n",
       "      <td>551422.jpg</td>\n",
       "      <td>./mincult-train/train/3712248/551422.jpg</td>\n",
       "      <td>./mincult-train/train/3712248/551422.jpg.embed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6339754</td>\n",
       "      <td>Сабля.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Оружие</td>\n",
       "      <td>2592073.jpg</td>\n",
       "      <td>./mincult-train/train/6339754/2592073.jpg</td>\n",
       "      <td>./mincult-train/train/6339754/2592073.jpg.embe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   object_id                                               name  \\\n",
       "0   10669820                Водолей - коник (фрагмент - голова)   \n",
       "1    4489444  Винтовка «Самозарядная винтовка Токарева» (мет...   \n",
       "2    8722586                           Инструмент. Калибр-скоба   \n",
       "3    3712248        Судомодель. НИС \"Космонавт  Виктор Пацаев\".   \n",
       "4    6339754                                             Сабля.   \n",
       "\n",
       "                                         description       group     img_name  \\\n",
       "0  сероглиняный, лепной, со сплошным белым ангобо...  Археология  7862029.jpg   \n",
       "1  На стволе имеется надульник, на  торце которог...      Оружие  9461061.jpg   \n",
       "2  Прямоугольная пластина с усечёнными углами и д...      Прочие  5095122.jpg   \n",
       "3   Корпус модели, надстройки, шлюпки выполнены и...      Прочие   551422.jpg   \n",
       "4                                                NaN      Оружие  2592073.jpg   \n",
       "\n",
       "                                         path  \\\n",
       "0  ./mincult-train/train/10669820/7862029.jpg   \n",
       "1   ./mincult-train/train/4489444/9461061.jpg   \n",
       "2   ./mincult-train/train/8722586/5095122.jpg   \n",
       "3    ./mincult-train/train/3712248/551422.jpg   \n",
       "4   ./mincult-train/train/6339754/2592073.jpg   \n",
       "\n",
       "                                      embedding_path  \n",
       "0  ./mincult-train/train/10669820/7862029.jpg.emb...  \n",
       "1  ./mincult-train/train/4489444/9461061.jpg.embe...  \n",
       "2  ./mincult-train/train/8722586/5095122.jpg.embe...  \n",
       "3  ./mincult-train/train/3712248/551422.jpg.embed...  \n",
       "4  ./mincult-train/train/6339754/2592073.jpg.embe...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CLIP(\n",
       "  (visual): VisionTransformer(\n",
       "    (conv1): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "    (ln_pre): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): Sequential(\n",
       "        (0): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (12): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (13): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (14): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (15): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (16): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (17): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (18): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (19): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (20): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (21): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (22): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (23): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (resblocks): Sequential(\n",
       "      (0): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (6): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (8): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (9): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (10): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (11): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (token_embedding): Embedding(49408, 768)\n",
       "  (ln_final): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import clip\n",
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"using {device}\")\n",
    "device = torch.device(device)\n",
    "model, preprocess = clip.load(\"ViT-L/14\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(image_path): \n",
    "    image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        return model.encode_image(image)[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "GEN_EMBEDDING = True\n",
    "if GEN_EMBEDDING: # if we need to generate embeddings from scratch\n",
    "    for index, row in df.iterrows(): \n",
    "        embedding = get_embedding(row.path)\n",
    "        with open(row.embedding_path, 'w') as f:\n",
    "            f.write(json.dumps(embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in ./venv/lib/python3.11/site-packages (1.13.0)\n",
      "Requirement already satisfied: numpy<2.3,>=1.22.4 in ./venv/lib/python3.11/site-packages (from scipy) (1.26.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "def find_best(image_path): \n",
    "    emb = get_embedding(image_path) \n",
    "    distances = {} \n",
    "    for index, row in df.iterrows(): \n",
    "        with open(row.embedding_path, 'r') as f:\n",
    "            other_emb = json.loads(f.readline())\n",
    "            dist = distance.cosine(emb, other_emb)\n",
    "            distances[index] = dist\n",
    "    dists = sorted(list(distances.items()), key=lambda a: a[1])[:10] \n",
    "    for i, dist in dists: \n",
    "        print(df.loc[[i]].path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'find_best' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfind_best\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mswordjpg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'find_best' is not defined"
     ]
    }
   ],
   "source": [
    "find_best('swordjpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ruclip.model as ruclip_model \n",
    "import ruclip.processor as ruclip_processor\n",
    "import ruclip.predictor as ruclip_predictor\n",
    "ru_clip_model = ruclip_model.CLIP.from_pretrained('./ruclip').eval()\n",
    "tokenizer = ruclip_processor.RuCLIPProcessor.from_pretrained('./ruclip')\n",
    "ru_predictor = ruclip_predictor.Predictor(ru_clip_model, tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0166,  0.0004, -0.0030,  ...,  0.0460, -0.0296, -0.0237],\n",
       "        [-0.0431, -0.0181, -0.0073,  ...,  0.0496, -0.0511,  0.0209],\n",
       "        [-0.0248, -0.0026, -0.0176,  ...,  0.0443, -0.0488, -0.0087],\n",
       "        [-0.0248, -0.0026, -0.0176,  ...,  0.0443, -0.0488, -0.0087],\n",
       "        [-0.0196, -0.0098, -0.0140,  ...,  0.0447, -0.0737, -0.0166]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ru_predictor.get_text_latents('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.53s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0434e-02,  4.4812e-02,  3.0217e-02, -5.0295e-02, -6.5523e-02,\n",
       "          4.0741e-02, -1.0987e-03,  4.5472e-02, -3.8732e-02,  1.4943e-02,\n",
       "         -5.2840e-02, -1.6329e-03,  9.4519e-03, -7.0657e-03, -4.5354e-02,\n",
       "         -1.4237e-02, -2.0416e-03, -4.6992e-03,  6.1217e-02,  1.5226e-02,\n",
       "          1.8209e-02, -1.1021e-02, -4.5310e-02, -4.1162e-03, -5.2625e-03,\n",
       "         -2.3020e-02,  1.7788e-03, -4.6810e-02, -1.4167e-02, -4.8316e-03,\n",
       "          2.7902e-02,  3.4978e-04,  6.2724e-02,  5.6399e-02, -7.8308e-02,\n",
       "         -3.7068e-02, -3.1299e-03, -3.4908e-02,  3.8411e-03,  1.6268e-02,\n",
       "         -3.7931e-02,  6.3340e-03,  4.1175e-02, -4.4566e-02, -3.0415e-02,\n",
       "         -4.2046e-02,  4.7650e-03,  2.8148e-02, -1.1778e-02, -2.1193e-02,\n",
       "          3.7772e-02,  4.9990e-03, -8.4161e-02,  1.3919e-02,  6.3200e-02,\n",
       "          5.4414e-02,  5.3479e-03, -1.6287e-02,  2.1855e-02,  7.1586e-03,\n",
       "          4.9568e-03,  5.1724e-03,  2.5335e-02,  1.2228e-02, -3.7275e-02,\n",
       "         -1.4031e-02, -9.5224e-03, -2.3642e-02,  3.0649e-02,  3.3941e-03,\n",
       "         -1.5349e-02, -7.3768e-03,  3.3826e-02, -6.0199e-02, -5.6529e-03,\n",
       "          9.4617e-03,  1.7663e-02,  9.6899e-03, -5.9191e-02,  9.7024e-03,\n",
       "          1.0380e-02, -5.4612e-02,  1.1162e-03,  4.4405e-03, -6.0353e-02,\n",
       "          1.3916e-03,  3.7799e-02, -2.8317e-02, -4.9847e-03, -5.1042e-02,\n",
       "          4.7874e-02,  7.6765e-03,  4.8473e-04, -9.5631e-02,  9.8795e-03,\n",
       "         -5.2856e-03, -2.4828e-03,  1.1012e-02,  2.1887e-02,  5.9653e-02,\n",
       "         -2.0536e-02,  3.5115e-02, -3.6345e-03, -2.5329e-02,  6.0192e-02,\n",
       "          2.1662e-02,  1.3918e-02, -2.3013e-02,  3.7640e-03, -4.2126e-02,\n",
       "          5.2935e-03,  6.6541e-03,  2.8181e-02,  7.1164e-03,  2.4355e-02,\n",
       "         -1.0958e-02, -4.8082e-02, -3.2255e-03, -7.9068e-02, -2.0301e-02,\n",
       "         -3.6968e-02,  6.0399e-02, -3.4543e-02, -3.7199e-02,  5.3066e-02,\n",
       "          5.3580e-03,  1.9282e-02,  6.7615e-04, -3.9931e-02,  1.0125e-02,\n",
       "          6.3687e-02,  5.2374e-02,  2.1583e-02,  2.5744e-02,  5.5406e-02,\n",
       "          4.2846e-02, -6.6051e-02, -3.4996e-02,  3.9039e-03,  9.0180e-02,\n",
       "         -5.3878e-03,  5.2620e-02,  2.4285e-02, -5.5021e-03,  1.2141e-02,\n",
       "          2.1013e-02,  3.0077e-02,  9.4100e-02, -1.7399e-02, -2.3426e-02,\n",
       "          4.5470e-02,  8.8704e-02,  6.1171e-02, -8.2585e-02, -3.1186e-02,\n",
       "         -5.2264e-03,  3.1984e-02, -2.6828e-02, -4.0709e-02, -6.6086e-02,\n",
       "         -3.3887e-02, -4.1309e-02,  2.6861e-02,  4.4497e-02,  9.4297e-03,\n",
       "         -1.6358e-02, -1.0585e-02, -2.7687e-02, -6.3933e-02, -8.3030e-02,\n",
       "         -6.7433e-03,  7.8523e-02,  4.0053e-02, -4.6518e-02,  1.3732e-03,\n",
       "          6.7563e-02,  1.6109e-02, -1.5954e-02,  4.0578e-02,  1.6546e-02,\n",
       "          1.8104e-02,  1.1902e-02,  4.7948e-02, -3.1416e-02, -2.2993e-02,\n",
       "          8.3279e-03,  9.0761e-03,  1.5162e-02, -8.9748e-03, -1.1465e-02,\n",
       "         -3.0482e-02,  2.6766e-02,  7.8767e-02, -4.6262e-02, -7.2819e-02,\n",
       "          1.8085e-02,  4.4694e-03, -6.1751e-03, -4.1491e-03, -1.3398e-02,\n",
       "          8.4942e-03, -2.0511e-03,  2.3705e-02,  1.3349e-02,  1.7255e-02,\n",
       "         -6.4725e-04, -6.4552e-03, -3.8376e-02,  9.1029e-03,  8.2073e-02,\n",
       "          4.8980e-03,  2.5734e-02,  6.0132e-02,  6.6166e-03, -3.0554e-03,\n",
       "         -4.0698e-02, -7.0547e-03, -1.1645e-02,  1.8996e-02, -3.5397e-02,\n",
       "         -5.8110e-02, -5.1866e-02,  1.3533e-02, -2.3213e-02, -3.2085e-02,\n",
       "          9.5974e-03,  1.2265e-02, -2.2452e-02,  3.0038e-02,  7.3237e-02,\n",
       "          4.8898e-03,  7.3670e-03,  4.5196e-02,  1.0083e-02, -4.4898e-02,\n",
       "          3.7715e-02, -3.8931e-02,  1.9155e-02,  3.8497e-02, -4.0675e-02,\n",
       "          1.7002e-02,  1.7542e-02,  8.5522e-03, -1.6233e-02, -3.7365e-02,\n",
       "          3.0131e-02, -6.5437e-02,  5.4495e-03,  2.4890e-03,  1.1033e-02,\n",
       "         -1.5222e-02, -2.7971e-02, -4.9756e-02,  2.2467e-03,  1.9042e-02,\n",
       "         -1.6568e-02,  6.9328e-03,  9.6744e-03, -3.2955e-02,  2.4480e-02,\n",
       "         -5.1680e-02,  1.4786e-02,  2.4954e-03,  1.8108e-02,  2.8258e-02,\n",
       "          6.4769e-04, -4.4184e-02, -9.4329e-03,  5.0435e-02, -4.8394e-03,\n",
       "          4.9153e-03, -4.6248e-02, -4.4617e-02, -1.6593e-02,  8.6291e-03,\n",
       "          2.3039e-02, -3.0279e-02, -4.2531e-02, -3.2177e-03,  9.9288e-03,\n",
       "         -2.3813e-02, -1.8640e-03, -3.2438e-02, -1.4001e-02,  9.3906e-03,\n",
       "          3.9150e-02, -3.1262e-02, -4.3308e-02,  1.2728e-02,  6.6546e-03,\n",
       "         -2.9018e-02,  2.9716e-02, -1.4380e-02, -3.5474e-02,  9.2968e-03,\n",
       "          1.9201e-02, -2.9420e-02,  5.6749e-02,  4.9914e-03, -2.9589e-02,\n",
       "          2.1679e-02,  4.9571e-02, -8.2631e-02, -4.4749e-02, -1.3861e-02,\n",
       "          2.0092e-02,  1.7550e-02,  4.9080e-03, -8.5855e-02, -6.1405e-03,\n",
       "          1.6171e-02, -2.8253e-02,  3.1612e-02,  1.2973e-02, -5.4777e-03,\n",
       "         -1.0354e-02,  1.8788e-02,  1.4658e-02, -3.2214e-02, -3.1670e-02,\n",
       "          1.2679e-02,  2.8107e-02,  7.1019e-03, -6.1481e-02,  2.9173e-03,\n",
       "         -7.8687e-02, -4.3281e-03, -4.2545e-03,  5.5904e-03, -1.8747e-02,\n",
       "         -2.4434e-02,  2.9078e-02, -1.4402e-03,  3.3900e-03, -5.2970e-02,\n",
       "          2.0833e-02,  1.0606e-02,  5.2867e-02, -3.8571e-02,  2.3144e-02,\n",
       "          2.6242e-02, -3.9645e-02, -8.3945e-03, -6.9319e-03,  1.7969e-03,\n",
       "         -1.3079e-03,  1.8237e-02,  5.8819e-02, -2.5726e-02,  2.2637e-02,\n",
       "          2.8817e-02,  6.0696e-03, -4.0562e-02, -4.5049e-02,  2.9003e-02,\n",
       "         -3.1917e-03, -8.7348e-02,  8.7559e-02, -1.4392e-02,  2.7863e-02,\n",
       "          4.0697e-03, -1.1263e-02,  5.1823e-02,  3.4773e-02, -3.0558e-02,\n",
       "         -2.4960e-02,  7.6505e-03,  2.9495e-02, -9.6222e-03, -1.9473e-03,\n",
       "         -5.0643e-02,  2.9104e-03,  1.0051e-03,  1.6971e-02,  2.3961e-02,\n",
       "          4.6770e-02, -3.2012e-02, -3.1894e-02, -1.0814e-02, -3.4195e-03,\n",
       "         -2.3884e-02,  2.4230e-02, -5.0627e-03,  5.2468e-02,  1.3215e-02,\n",
       "          5.0029e-03, -1.9323e-02, -7.9188e-03,  2.3483e-02,  7.5837e-02,\n",
       "         -3.4988e-02, -4.7697e-05, -3.6867e-02, -2.7288e-02,  5.4255e-03,\n",
       "          7.1071e-03,  7.1958e-02,  5.3094e-02,  1.6870e-02,  4.9888e-02,\n",
       "          8.0889e-03, -8.1024e-03,  2.0483e-03, -7.1255e-02,  5.6649e-03,\n",
       "         -2.8292e-02, -4.4367e-02, -1.6827e-02, -1.6973e-02,  5.0677e-02,\n",
       "         -1.7412e-02, -3.9119e-02, -1.7003e-02,  2.8509e-02, -1.3509e-02,\n",
       "         -2.4580e-02, -3.7460e-03, -1.5102e-02, -9.9172e-02, -3.8147e-02,\n",
       "         -2.2449e-02, -1.7031e-02,  3.0909e-02,  1.6461e-03, -5.3940e-03,\n",
       "          5.1355e-03, -2.3147e-02, -2.6514e-02,  3.8223e-03,  1.7400e-02,\n",
       "          8.9918e-02, -2.3101e-02,  2.1625e-02,  1.1342e-02,  2.9165e-02,\n",
       "          6.1182e-02,  1.3918e-02, -5.2674e-03, -5.7079e-02, -6.2757e-02,\n",
       "          3.9600e-02, -1.6695e-02,  1.4528e-02,  1.2021e-02, -5.5846e-02,\n",
       "         -1.5411e-02,  5.4902e-02, -2.2557e-02,  1.2658e-02,  6.4384e-02,\n",
       "         -1.1779e-02, -7.0588e-02,  2.1343e-02,  1.0194e-02,  6.1023e-02,\n",
       "          3.3401e-02,  3.9088e-02, -1.6265e-02, -3.1276e-02, -2.9901e-02,\n",
       "         -5.2171e-02,  2.3011e-02, -8.2745e-03, -8.5040e-03,  1.4414e-02,\n",
       "          4.3413e-02, -3.5957e-03,  3.0360e-04, -3.4241e-02,  3.3416e-03,\n",
       "         -1.9054e-02,  2.9629e-02, -3.0437e-02, -4.1614e-03, -2.5877e-02,\n",
       "         -2.8250e-02, -2.8711e-02,  1.1935e-02, -2.2211e-02,  3.5760e-02,\n",
       "          1.4555e-02,  2.2094e-02,  5.7815e-03, -5.4739e-02, -3.0174e-02,\n",
       "          2.3953e-03, -7.2565e-02, -1.5716e-02, -5.5126e-02, -5.1848e-02,\n",
       "         -5.1620e-02, -2.0173e-02, -2.3175e-02,  6.5457e-03,  2.2279e-02,\n",
       "          1.7725e-03, -4.1414e-02,  4.7468e-02,  4.2059e-02,  5.8127e-02,\n",
       "          2.4873e-02,  4.5488e-02,  1.8131e-02,  2.0573e-02, -5.0563e-03,\n",
       "         -7.0757e-02, -2.3966e-03, -6.5797e-03,  2.9916e-02,  4.8377e-03,\n",
       "          1.8709e-02, -2.6610e-02, -2.1801e-02,  2.5201e-02,  3.5422e-03,\n",
       "          2.8505e-02, -2.7486e-02,  2.4185e-03,  2.7324e-02,  9.7166e-03,\n",
       "          5.7898e-02, -1.0087e-02, -4.7137e-04,  1.4259e-02,  2.5729e-02,\n",
       "          1.0768e-02,  4.7691e-02, -2.6538e-02, -5.0952e-02,  3.4086e-02,\n",
       "         -2.7087e-02, -3.0281e-02,  3.5154e-02, -2.7465e-02, -3.1202e-02,\n",
       "          4.8908e-02,  6.4464e-02, -1.1269e-02,  9.2032e-03, -7.9568e-03,\n",
       "          8.6689e-02,  3.2836e-02,  2.4020e-02,  2.2951e-02, -5.9318e-02,\n",
       "          1.2793e-02,  9.5552e-04, -1.6048e-02,  3.2631e-02, -1.4206e-02,\n",
       "          3.9729e-02, -3.1892e-02, -1.9558e-02,  5.5910e-03, -6.6407e-02,\n",
       "         -3.2319e-02, -1.2795e-02,  1.1573e-02, -5.4980e-02,  6.0098e-04,\n",
       "          2.3242e-02, -1.4508e-02, -7.3985e-03, -2.8786e-02, -4.9655e-02,\n",
       "          3.0560e-02,  1.3011e-02,  3.1966e-02, -1.4138e-02,  7.0563e-02,\n",
       "          8.6332e-02,  5.8453e-03,  6.1372e-02,  2.6468e-02,  6.2002e-02,\n",
       "         -5.9233e-02,  3.8475e-02, -2.2312e-02,  2.7273e-02,  3.6005e-02,\n",
       "         -3.3937e-02,  1.0911e-02,  1.9916e-02, -4.4829e-03,  2.7082e-02,\n",
       "         -8.9283e-02, -6.3266e-02,  1.1393e-02, -1.7867e-02, -1.3050e-02,\n",
       "         -2.8476e-02, -8.6516e-03, -1.4531e-02,  9.0017e-03, -5.4368e-02,\n",
       "          1.7140e-02, -2.3178e-02,  2.8548e-02,  9.1961e-02, -1.3868e-02,\n",
       "          5.9178e-03,  8.0210e-02,  2.0443e-02,  3.7400e-02, -7.7934e-02,\n",
       "         -4.0227e-02, -3.0032e-02, -1.7385e-02,  1.7503e-02,  4.7289e-03,\n",
       "         -5.0788e-02,  4.4030e-02,  1.8780e-02, -4.0384e-02, -3.2334e-02,\n",
       "          2.5805e-02, -2.8761e-02,  2.5027e-02,  5.9540e-02, -4.5990e-02,\n",
       "          2.6876e-02,  4.2793e-02,  1.0159e-02,  4.1784e-03,  9.5014e-03,\n",
       "          9.5677e-02,  2.4725e-02,  2.1949e-02,  1.8595e-02,  4.0751e-02,\n",
       "          1.6987e-02,  4.2424e-03,  2.7013e-03, -2.0586e-02, -5.7324e-02,\n",
       "         -4.8258e-02,  2.5527e-02, -2.4663e-02,  1.0237e-01,  6.3267e-02,\n",
       "          1.6722e-02,  5.0042e-02, -3.7121e-02,  1.1493e-02,  8.8218e-02,\n",
       "          1.3317e-02, -4.6513e-02,  2.0418e-03,  7.6407e-04,  3.0232e-02,\n",
       "          3.9392e-02, -8.4746e-02, -3.0646e-02,  3.3216e-02,  5.1303e-03,\n",
       "          1.2967e-02,  3.4275e-02,  2.2043e-02, -6.7716e-02, -1.9258e-02,\n",
       "         -7.6986e-02, -3.1126e-02,  1.9966e-02, -2.1719e-02,  3.3120e-02,\n",
       "         -1.4516e-02, -2.1509e-02, -1.2735e-02,  3.3395e-02,  1.1348e-02,\n",
       "         -1.0322e-02,  3.0363e-02, -4.1591e-02, -3.1673e-02,  7.1555e-02,\n",
       "          3.3521e-03, -5.9408e-02,  4.1521e-02, -1.2174e-02, -4.0623e-02,\n",
       "         -1.7336e-02, -1.7078e-02, -2.1255e-04, -3.0966e-02,  1.4817e-02,\n",
       "          4.5465e-03, -2.5591e-02,  7.0501e-02, -1.3414e-02,  1.5978e-02,\n",
       "         -5.3281e-02,  9.4499e-03,  3.7519e-02, -1.8884e-03,  2.5621e-02,\n",
       "          3.7411e-02, -1.2498e-02,  6.2602e-02, -3.7183e-02,  3.0869e-02,\n",
       "         -6.5069e-02,  4.4846e-02,  1.0134e-02,  5.9754e-02,  1.7184e-02,\n",
       "         -1.0219e-02,  1.6337e-02, -2.3838e-02,  5.7482e-02,  1.0228e-02,\n",
       "         -5.1780e-02,  7.3965e-02,  7.1625e-02,  3.3128e-02, -5.8286e-02,\n",
       "         -9.1896e-02, -5.6037e-02,  1.9844e-02, -1.5778e-02,  1.3163e-02,\n",
       "         -5.7673e-03, -2.1236e-02,  5.9878e-03, -9.0731e-04, -8.2714e-02,\n",
       "         -5.1617e-02, -2.8115e-02, -1.5839e-02, -5.0368e-02, -2.8102e-02,\n",
       "         -6.5627e-02,  5.5538e-02, -3.2409e-02,  5.6203e-02,  4.9301e-02,\n",
       "         -7.7310e-03,  5.1959e-02,  2.0882e-02,  4.6314e-02, -9.0301e-02,\n",
       "         -2.6065e-02, -7.6169e-03, -4.9291e-02,  1.1423e-02, -4.3044e-02,\n",
       "          5.7132e-02, -3.9001e-02, -8.3033e-02,  2.0711e-02,  5.2556e-02,\n",
       "         -3.1364e-02, -3.6949e-02, -1.4019e-02, -3.5105e-02,  6.3487e-02,\n",
       "         -1.0137e-02,  3.1946e-02,  3.5226e-03, -2.9125e-02, -2.3998e-02,\n",
       "          2.5186e-02, -4.7281e-02,  4.2235e-02,  3.2693e-02, -1.1511e-02,\n",
       "         -2.1441e-03, -1.6694e-02,  6.4723e-02]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ru_predictor.get_image_latents([Image.open('test_image.jpg')])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
